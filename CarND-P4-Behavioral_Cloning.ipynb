{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Self-Driving Car Engineer Nanodegree\n",
    "## Project 4: (Deep Learning) Driving Behavioral Cloning\n",
    "\n",
    "In this project, I used what I've learned about deep neural networks and convolutional neural networks to clone driving behavior. I trained, validated and tested a model using Keras. The model will output a steering angle to an autonomous vehicle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras library is used here because it makes it super easy to arrange the necessary building blocks without spending a lot of time writing boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing useful packages\n",
    "from utils import load_dataset, plot_data_distribution, plot_training_history\n",
    "from utils import create_video, reproduce_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Dataset Loading, Summary and Exploration\n",
    "\n",
    "Te data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of car's cameras\n",
    "- `'y_train'` is a 1D array containing the stering value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "CORRECTION = 0.25 # Correction factor for stearing angles\n",
    "DOWN_LIMIT = 20 # Down Horizontal limits to crop dataset\n",
    "BATCH_SIZE = 128 # Batch size for training\n",
    "UP_LIMIT = 65 # Up Horizontal limits to crop dataset\n",
    "EPOCHS = 20 # NUmber of epochs for traings process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Basic Summay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From .... iving-data-capture/stage_1_(1): 8422 samples\n",
      "/media/john/Data/NanoDegree/Projects/CarND-P4-Behavioral_Cloning/self-driving-data-capture/stage_1_(1)/IMG/left_2019_04_03_00_34_47_232.jpg\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/home/john/opencv/modules/highgui/src/window.cpp:325: error: (-215) size.width>0 && size.height>0 in function imshow\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c51372270edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m reproduce_dataset(fps=30, loop=True, sc_fc=1.5, up_limit=UP_LIMIT, down_limit=DOWN_LIMIT, \n\u001b[0;32m---> 12\u001b[0;31m                   dataset_path = dataset_paths[0], CORRECTION=CORRECTION)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# # Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/john/Data1/NanoDegree/Projects/CarND-P4-Behavioral_Cloning/utils.py\u001b[0m in \u001b[0;36mreproduce_dataset\u001b[0;34m(fps, loop, sc_fc, up_limit, down_limit, dataset_path, CORRECTION)\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"caca\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mup_limit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdown_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                         \u001b[0;31m# Draw superior limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /home/john/opencv/modules/highgui/src/window.cpp:325: error: (-215) size.width>0 && size.height>0 in function imshow\n"
     ]
    }
   ],
   "source": [
    "# Define paths \n",
    "dataset_paths = [\n",
    "    \"self-driving-data-capture/stage_1_(1)\",\n",
    "    \"self-driving-data-capture/stage_1_(2)\",\n",
    "#     \"self-driving-data-capture/stage_1_(3)\",\n",
    "    \"self-driving-data-capture/stage_2_(1)\",\n",
    "#     \"self-driving-data-capture/stage_2_(2)\",\n",
    "    \"self-driving-data-capture/stage_2_(3)\"\n",
    "    ]\n",
    "\n",
    "reproduce_dataset(fps=30, loop=True, sc_fc=1.5, up_limit=UP_LIMIT, down_limit=DOWN_LIMIT, \n",
    "                  dataset_path = dataset_paths[0], CORRECTION=CORRECTION)\n",
    "\n",
    "# # Load dataset\n",
    "# data = load_dataset(dataset_paths,  \"driving_log.csv\")\n",
    "# data = shuffle(data)\n",
    "\n",
    "# # Separate samples for training and testing process\n",
    "# train_samples, validation_samples = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-91ed94a32025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"media/john/Data1/NanoDegree/Projects/CarND-P4-Behavioral_Cloning/self-driving-data-capture/stage_1_(1)/IMG/left_2019_04_03_00_34_47_232.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "A = cv2.imread(\"media/john/Data/NanoDegree/Projects/CarND-P4-Behavioral_Cloning/self-driving-data-capture/stage_1_(1)/IMG/left_2019_04_03_00_34_47_232.jpg\")\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print dataset information\n",
    "print(\"Number of training samples: {}\".format(len(train_samples))) # Number of training examples\n",
    "print(\"Number of testing samples: {}\".format(len(validation_samples))) # Number of testing examples\n",
    "print(\"Number of Dataset samples: {}\".format(len(data))) # Number of training examples\n",
    "\n",
    "# Plot dataset distribution\n",
    "y_data_raw = np.asarray([data[idx][\"steering\"] for idx in range(len(data))])\n",
    "plot_data_distribution(y_data_raw, scfc=30, graph_name=\"Distribution of dataset\", save_name=\"writeup_files/dataset_stee_angles.png\")\n",
    "\n",
    "# Plot dataset distribution\n",
    "y_train_raw = np.asarray([train_samples[idx][\"steering\"] for idx in range(len(train_samples))])\n",
    "plot_data_distribution(y_train_raw, scfc=30, graph_name=\"Distribution of training set\",  save_name=\"writeup_files/training_stee_angles.png\")\n",
    "\n",
    "# Plot dataset distribution\n",
    "y_test_raw = np.asarray([validation_samples[idx][\"steering\"] for idx in range(len(validation_samples))])\n",
    "plot_data_distribution(y_test_raw, scfc=30, graph_name=\"Distribution of testing set\",  save_name=\"writeup_files/testing_stee_angles.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32, angle_fc = 0.2):\n",
    "    \n",
    "    batch_size_org = batch_size\n",
    "    batch_size = int(np.ceil(batch_size/6))\n",
    "    \n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        samples = shuffle(samples)\n",
    "        \n",
    "        for offset in range(0, len(samples), batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images = []; angles = []\n",
    "            \n",
    "            # Read image and steering angle\n",
    "            for batch_sample in batch_samples:\n",
    "                                                    \n",
    "                # Central camera\n",
    "                img_c = cv2.cvtColor(cv2.imread(batch_sample[\"img_c\"]), cv2.COLOR_BGR2RGB) \n",
    "                ang_c =  batch_sample[\"steering\"]\n",
    "                \n",
    "                # Left camera\n",
    "                img_l = cv2.cvtColor(cv2.imread(batch_sample[\"img_l\"]), cv2.COLOR_BGR2RGB)\n",
    "                ang_l =  batch_sample[\"steering\"] + angle_fc\n",
    "\n",
    "                # Right camera\n",
    "                img_r = cv2.cvtColor(cv2.imread(batch_sample[\"img_r\"]), cv2.COLOR_BGR2RGB)\n",
    "                ang_r =  batch_sample[\"steering\"] - angle_fc\n",
    "\n",
    "                # Fliped Left camera\n",
    "                img_lf = cv2.flip(img_l, 1)\n",
    "                ang_lf = -ang_l\n",
    "\n",
    "                # Fliped Right camera\n",
    "                img_rf = cv2.flip(img_r, 1)\n",
    "                ang_rf = -ang_r\n",
    "\n",
    "                # Fliped center camera\n",
    "                img_cf = cv2.flip(img_c, 1)\n",
    "                ang_cf = -ang_c \n",
    "\n",
    "                images.extend((img_c, img_l, img_r, img_cf, img_lf, img_rf))\n",
    "                angles.extend((ang_c, ang_l, ang_r, ang_cf, ang_lf, ang_rf))\n",
    "                      \n",
    "            images, angles = shuffle(images, angles)\n",
    "                    \n",
    "            # trim image to only see section with road\n",
    "            X_train = np.array(images[:32])\n",
    "            y_train = np.array(angles[:32])\n",
    "                        \n",
    "            # Return batch\n",
    "            yield shuffle(X_train, y_train)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = generator(train_samples, BATCH_SIZE, CORRECTION)\n",
    "validation_generator = generator(validation_samples, BATCH_SIZE, CORRECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create videos with current datasets\n",
    "path_idx = 0\n",
    "for cam_label in [\"img_c\", \"img_l\", \"img_r\"]:\n",
    "    create_video(cam_label=cam_label, \n",
    "                 dst_path=\"model/video_results\", \n",
    "                 src_path=dataset_paths[path_idx],\n",
    "                 file_name=\"{}_{}.mp4\".format(dataset_paths[path_idx].split(\"/\")[-1], cam_label),\n",
    "                 video_size=(320, 160),\n",
    "                 fps=15.\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate cameras videos and reproduce  -> A: Stop video - Q: Quit\n",
    "reproduce_dataset(fps=30, loop=True, sc_fc=1.5, up_limit=UP_LIMIT, down_limit=DOWN_LIMIT, \n",
    "                  dataset_path = dataset_paths[2], CORRECTION=CORRECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Designing, Training and Testing a Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# # Preprocess incoming data, centered around zero with small standard deviation \n",
    "model.add(Lambda(lambda x: (x/255.) - 0.5, input_shape=(160, 320, 3)))\n",
    "\n",
    "# # trim image to only see section with road\n",
    "model.add(Cropping2D(cropping=((DOWN_LIMIT, UP_LIMIT),(0, 0))))\n",
    "\n",
    "#layer 1- Convolution, no of filters- 24, filter size= 5x5, stride= 2x2\n",
    "model.add(Convolution2D(24, 5, 5, subsample=(2,2), activation=\"relu\"))\n",
    "\n",
    "#layer 2- Convolution, no of filters- 36, filter size= 5x5, stride= 2x2\n",
    "model.add(Convolution2D(36, 5, 5, subsample=(2,2), activation=\"relu\"))\n",
    "\n",
    "#layer 3- Convolution, no of filters- 48, filter size= 5x5, stride= 2x2\n",
    "model.add(Convolution2D(48, 5, 5, subsample=(2,2), activation=\"relu\"))\n",
    "\n",
    "#layer 4- Convolution, no of filters- 64, filter size= 3x3, stride= 1x1\n",
    "model.add(Convolution2D(64, 3, 3, activation=\"relu\"))\n",
    "\n",
    "#layer 5- Convolution, no of filters- 64, filter size= 3x3, stride= 1x1\n",
    "model.add(Convolution2D(64, 3, 3, activation=\"relu\"))\n",
    "\n",
    "# model.add(MaxPooling2D())\n",
    "\n",
    "#flatten image from 2D to side by side\n",
    "model.add(Flatten())\n",
    "\n",
    "#layer 6- fully connected layer 1\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "#Adding a dropout layer to avoid overfitting. Here we are have given the dropout \n",
    "# rate as 25% after first fully connected layer\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#layer 7- fully connected layer 1\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "\n",
    "#layer 8- fully connected layer 1\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "\n",
    "#layer 9- fully connected layer 1\n",
    "#here the final layer will contain one value as this is a regression problem and not classification\n",
    "model.add(Dense(1))\n",
    "\n",
    "# the output is the steering angle using mean squared error loss function is the right choice \n",
    "# for this regression problem adam optimizer is used here\n",
    "optimizer = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "history_object  = model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=math.ceil(len(train_samples*6)/BATCH_SIZE), \n",
    "        validation_data=validation_generator, \n",
    "        validation_steps=math.ceil(len(validation_samples)/BATCH_SIZE), \n",
    "        epochs=EPOCHS, \n",
    "        verbose=1)\n",
    "\n",
    "# Save the model whe n finish\n",
    "model.save('model/behavioral_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# keras method to print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_object, fig_size=(13,8), save_name=\"writeup_files/training_loss_graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Simulation Enviroment, Model and Other Utils\n",
    "NOTE: If you recently trained a model you must restart the kernel to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setClipboardData, create_model_result_video\n",
    "from IPython.display import clear_output\n",
    "from multiprocessing import Pool \n",
    "import os\n",
    "\n",
    "def run_process(process):                                                             \n",
    "    os.system('{}'.format(process)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Video from desktop\n",
    "video_name = \"behavioral_driving_stg{}_test{}.mp4\".format(1, 1)\n",
    "command = \"ffmpeg -video_size {}x{} -framerate 25 -f x11grab -i :0.0+100,200 {}\".format(\n",
    "    1000, 650, os.path.join(os.getcwd(), \"model/video_results\", video_name))\n",
    "print(command); setClipboardData(command.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize video\n",
    "command = \"ffmpeg -video_size {}x{} -framerate 25 -f x11grab -i :0.0+100,200 {}\".format(\n",
    "    1000, 650, os.path.join(os.getcwd(), \"model/video_results\", video_name))\n",
    "print(command); setClipboardData(command.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gif from videovideo_name, \n",
    "video_name = \"stage_1_03312019_img_r\"\n",
    "command = \"ffmpeg -i {}.mp4 -ss 15 -t 20 {}.gif -hide_banner\".format(video_name, video_name)\n",
    "print(command); setClipboardData(command.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulator and socket to drive autonomously\n",
    "try:\n",
    "    time_for_server = 10\n",
    "    processes = (\n",
    "        \"clear && {}\".format(os.path.join(os.getcwd(), \"beta_simulator_linux\", \"beta_simulator.x86_64\")),\n",
    "        \"clear && termdown {} && clear && python3 {} {} {}\".format(\n",
    "            time_for_server,\n",
    "            os.path.join(os.getcwd(), \"drive.py\"), \n",
    "            os.path.join(os.getcwd(), \"model\", \"behavioral_model.h5\"), \n",
    "            os.path.join(os.getcwd(), \"model\", \"image_results \")))\n",
    "    print(processes[0])\n",
    "    print(processes[1])\n",
    "    pool = Pool(processes=len(processes))                                                        \n",
    "    pool.map(run_process, processes)\n",
    "except:\n",
    "    print(\"Process has finished\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only simulation environment\n",
    "command = os.path.join(os.getcwd(), \"beta_simulator_linux\", \"beta_simulator.x86_64\")\n",
    "os.system('{}'.format(command)) \n",
    "print(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model's results video   \n",
    "create_model_result_video(src_path = os.path.join(os.getcwd(), \"model\", \"image_results\"), \n",
    "                          dst_path = os.path.join(os.getcwd(), \"model\", \"video_results\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
